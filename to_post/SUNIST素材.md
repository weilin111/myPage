---
layout: post
title: 迷思&随想
date: 2020-8-16 00:00:00 +0800
category: ~~~~~~
thumbnail: style/image/postLogo/图片7.png
icon: book
cate: Fun
---

* content
{:toc}


## 值得注意的点

关于数据的完备性     
以及其因果关系   控制信号？ 诊断信号？
关于数据时间轴的对其问题  数据标准化
非破裂放电的数据？

典型波形 及其 物理对应   SUNIST更多的知识文档

先验 预先知道的知识是什么  例如电流的范围

meta-parameter的问题，如何从MPs复原控制信号

预训练及训练的技术  正则化  如何找合适的模型复杂度


用transformer吗  模型复杂度的问题

服务器的问题  可以给别人用

问题有哪些痛点

## 可能的目的与思路
1. 一个较高准确度的神经网络模型  
1.2 对不同结构对比
2. 基于MPs生成输入序列，做参数搜寻来优化放电波形
3. 实用性，工作做完之后能给SUNIST运行的参数做参考
4. 可解释性与物理对应   
5. 对于在SUNIST(或类似装置)中应用编码解码结构的研究
6. 

## 介绍


### SUNIST

中国联合球型托卡马克SUNIST(Sino United Spherical Tokamak)是我国第一台球型托卡马克实验装置，于2002年建成。装置主要设计参数为：大半径0.3m，小半径0.23m，然拉长比1.6，中心磁场0.15T和等离子体电流50kA，放电时间在毫秒量级。SUNIST装置的主体包括真空室和脉冲磁体系统。磁体系统由环向场线圈、欧姆加热线圈和平衡场线圈组成。此外，还包括供电系统、真空系统、诊断系统、控制系统和数据采集系统等。它的目标是进行低环径比环形等离子体物理的研究和采用非感应加热和驱动的方法产生可维持的种子等离子体。

诊断系统的组成部分有：Mirnov磁探针、磁通环、经典探针、金硅面垒二极管阵列、微波干涉仪。


### SUNIST放电数据

典型波形 及其 物理对应

### 机器学习@tokamak

对于托卡马克装置的建模分析是核聚变研究中的重要课题。传统的方法是物理驱动的，在第一性原理下建立物理模型，推导（通常在，在一些简化与理想的条件下）。随着计算机的发展，我们可以处理更高的复杂度，在模拟当中可以考虑更多的物理因素(输运、平衡、稳定性、边界物理、加热、加料、电流驱动)，即所谓的"Integrated Modeling"。典型的工作了有ETS [1], PTRANSP [2], TSC [3], CRONOS [4], JINTRAC [5], METIS [6], ASTRA [7], TOPICS。从第一性原理出发，所考虑的物理过程的完备性决定了结果的准确性。然而托卡马克放电过程具有非线性、多尺度、多物理的特点，要的可靠的模拟结果仍然是充满挑战的课题。

复杂系统


另外一种行之有效的方法是由数据驱动的方法，随着机器学习技术的发展，尤其是其在自然语言处理当中的应用。NLP的模型特点是序列到序列的，例如机器翻译、语音识别、问答系统等。神经网络逐渐被应用到聚变领域当中来，例如破裂(disruption)预测、放电预测等。聚变中的神经网络建模，大多是时间序列到时间序列的问题，例如给定磁场(控制信号)输出放电波形(诊断信号)。许多的诊断信号大多是一个一维的函数分布。物理驱动的方法通常是将由简入繁地建立高维的方程，然后化简到低维去分析。与之不同，数据驱动的方法则是从已有的大量数据中提取出低维的量之间的映射关系，进而得到非线性系统的估计模型。对于复杂系统来说，数据驱动更为有效并实用。递归神经网络(RNN)及其变体在序列到序列(Seq2Seq)的建模中是常用而有效的。根据应用特点即其数据特点确定网络结构后，在标注好的数据集上，通过反向传播的算法(BP)对网络进行训练，完成对数据集的建模。

### RNN LSTM 编码解码 Transformer
根据Seq2Seq任务的特点，对于前后关系、上下文关系的建模是一大关注点。
神经网络由一层一层的神经元堆叠组成，层内、曾与曾之间的各个神经元存在联系。数据由输入层进入网络，前向传播完成运算，在输出层输出，这个过程被称为前向传播。通过损失函数(loss function)将输出层的输出(预测)与目标进行比较，损失越大则预测与目标偏离较远。通过反向传播算法来优化损失函数，找到其极值点，使得预测与目标相符合，这个过程被称为神经网络的训练。
循环神经网络(RNN)的神经元是具有自反馈的神经元，这种结构的优点是使得网络具有动态特性，即后面时刻会受到前面时刻的状态(记忆)。这种前后影响的关系有利于其在Seq2Seq中的应用。其神经元的权重参数对于所有时刻是共享的。RNN的输入是一些高维矢量的序列(例如时间序列)，在聚变应用当中可以是许多诊断信号集合的序列。输入和输出序列可以是任意长度的，即序列到序列中可以是一对一、一对多、多对多的，根据任务特点而定。
在NLP当中，这些高维矢量通常是对于词汇的表示(embedding，词向量),即通过一些映射将词汇库的词汇映射为高维的词向量。而聚变研究中，对于神经网络输入的表示不需要做这样的embedding，信号的集合可以直接作为高维矢量。RNN的输入输出序列的长度不一定必须相等。RNN也存在一些不足之处，因此人们提出了RNN的各种变体，例如LSTM、GRN、注意力机制、自注意力等。RNN具有这样的特点，对于下一时间点的预测，时间范围较远的影响比较小，而有时那些较远的信息可能对预测来说是关键的。LSTM通过引入门机制解决了RNN长时间依赖的问题，使得网络具有长期记忆与遗忘的能力。GRU，则是对于LSTM的一种优化，减少了逻辑门的复杂度，降低计算开销。注意力机制则是通过对不同序列点之间的关系进行度量来提升模型能力，即进一步提升模型的记忆能力。

RNN一般的结构通常是输入层、隐藏层、输出层。其中隐藏层的神经元即是前面提到的具有自反馈特点的神经元，其输出分为两个部分，一部分流向输出层、一部分反馈到自身用于序列下一时刻的计算(更新隐藏层状态)。具有多个隐藏层的RNN则被称为多层RNN。人们发现网络深度的增加有利于建模，即深度学习，多层RNN的结构能够提升模型的效果。


LSTM

裸RNN与 编码 解码 的区别：
RNN：直接循环演算生成序列
编码解码：先提取特征(编码)，再使用特征来循环生成序列(解码)


Transformer长序列运算开销极大  
考虑上下文 注意力机制    多头注意力
一般的翻译任务，一个句子通常通常包含十几二十个单词，序列长度并不很长。而在聚变应用，序列长度相对要长的多。

dropout 防止过拟合

### 关于编码与解码的结构 NLP

对于输入的高维表示

mask 并行化


## 模型与方法


## 优化参数搜索

## 结果

### 拟合度


## 结论

### 展望


## 文章

### EAST文章

EAST 诊断数据 >3000通道  
模型：LSTM的编码解码结构
数据集：3476 shot （6：2：2）非破裂的数据  

控制信号 到 诊断信号
68通道  65输入[等离子电流 PF电流...]  3输出 [环电压 储能  电子数密度 ]

input序列长度分段降采样 

只是预测，并没有参数搜索

### TAE 文章 Achievement of Sustained Net Plasma Heating in a Fusion Experiment with the Optometrist Algorithm

本文介绍了Google赞助的TAE公司如何在其聚变实验装置上应用优化算法对等离子体参数的提升。文章提出了meta-parameter(MPs)的概念，即将控制信号用一些meta-parameter来概括。其算法被称为验光师算法，大概的过程为，0）设定参考参数水平，1)通过随机算法由原来MPs计算新的MPs,2)以MPs做实验（几次），3)人为判断实验的参数水平是否有提升(如果提升则接受这个MPs，并设置新的参考)，4）重复上述步骤。TAE是，边做实验边搜索参数空间的，非神经网络。其实验取得较好结果，等离子体温度上升，能损减少。


### JET disruption predictor [2019 10]  [Deep Learning for Plasma Tomography and Disruption Prediction from Bolometer Data]

托卡马克放电过程中可能会出现等离子体破裂，大量的储能快速释放到周围的设备上，可能会造成设备损坏。破裂预测、预警是保证装置安全运行的一项重要工作。
数据集： 输入bolometer的16通道前1秒内的数据(16*200)   输出：当前时间点发生破裂的概率(分类任务) or 距离破裂放生的时间(回归任务)
网络结构以CNN作为编码器(提取特征)，后接LSTM，全连接层输出标量


### 长序列Transformer  [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting]



## 数据复杂度的问题























